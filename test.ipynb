{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, chars, vocab_size) -> None:\n",
    "        self.chars = chars\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        return [stoi[c] for c in text] \n",
    "\n",
    "    def decode(self, integers: List[int]) -> str: \n",
    "        itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        return ''.join([itos[i] for i in integers]) \n",
    "\n",
    "class OpenAITokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.vocab_size = self.enc.n_vocab\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, lst: List[int]) -> str: \n",
    "        return ''.join([self.enc.decode(el) for el in lst])  \n",
    "    \n",
    "class ShakespareDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size):\n",
    "        self.data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+self.block_size+1]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5pnrshs4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-galaxy-1</strong> at: <a href='https://wandb.ai/minigpttu/mini-GPT/runs/5pnrshs4' target=\"_blank\">https://wandb.ai/minigpttu/mini-GPT/runs/5pnrshs4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240308_110357-5pnrshs4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5pnrshs4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\thoma\\OneDrive - Delft University of Technology\\Documenten\\UNI\\Bayesian\\Assignment 2\\MiniGPT\\wandb\\run-20240308_110414-uu2r0jaf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/minigpttu/mini-GPT/runs/uu2r0jaf' target=\"_blank\">logical-vortex-2</a></strong> to <a href='https://wandb.ai/minigpttu/mini-GPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/minigpttu/mini-GPT' target=\"_blank\">https://wandb.ai/minigpttu/mini-GPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/minigpttu/mini-GPT/runs/uu2r0jaf' target=\"_blank\">https://wandb.ai/minigpttu/mini-GPT/runs/uu2r0jaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/minigpttu/mini-GPT/runs/uu2r0jaf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1d0b2f2c290>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    project=\"mini-GPT\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, dataloader, eval_iters):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = next(iter(dataloader))\n",
    "        # compute loss\n",
    "        logits, loss = model(X, Y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return np.mean(losses)\n",
    "\n",
    "def train(train_loader, test_loader, model, optimizer, max_iters, eval_interval, eval_iters):\n",
    "    for i in range(max_iters):\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "            train_loss = estimate_loss(model, train_loader, eval_iters)\n",
    "            val_loss = estimate_loss(model, test_loader, eval_iters)\n",
    "            wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "            # print(f\"step {i}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "    \n",
    "        batch = next(iter(train_loader))\n",
    "        x, y = batch\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1337\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# read file.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# create tokenizer.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input.txt'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    " \n",
    "# read file.\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# create tokenizer.\n",
    "chars = sorted(list(set(text))) \n",
    "vocab_size = len(chars)\n",
    "\n",
    "tokenizer = Tokenizer(chars, vocab_size)\n",
    "# tokenizer = OpenAITokenizer()\n",
    "\n",
    "# train and test splits\n",
    "dataset = ShakespareDataset(text, tokenizer, block_size)\n",
    "n = int(0.9*len(dataset)) # first 90% will be train, rest val\n",
    "train_dataset, test_dataset = torch.utils.data.dataset.random_split(dataset, [n, len(dataset) - n])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# create model.\n",
    "model = BigramLanguageModel(vocab_size=tokenizer.vocab_size)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(train_loader, test_loader, model, optimizer, max_iters, eval_interval, eval_iters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PETNO:\n",
      "Iull to eve vides teern end, chiss flare you of theave?\n",
      "\n",
      "\n",
      "SUCUNTUK:\n",
      "A, shal irn st, hen lordeng for hondeed,\n",
      "Thou thun son will foom, govy that trootcioug Eurd, ast depesk.\n",
      "\n",
      "OLI:Cer'sp ap. I I ssufflores whoused for be hich; my sut leank?\n",
      "\n",
      "INIO:\n",
      "Shay:\n",
      "Makht fat you?\n",
      "\n",
      "\n",
      "NUNG IO:\n",
      "Co dhe enop arsit's wice of know,\n",
      "Tre thirthe his tnobe hou that sirest dod reare, asd chell youk.\n",
      "\n",
      "LOUBEERDNE:\n",
      "Whils Ry of for le.\n",
      "\n",
      "\n",
      "Thorse here's whage tho is doug Qpe hereve tasis to ou dry our,\n",
      "cwheraurt ill: noto hat the to beek bro,\n",
      "For not pagse know sto the do his-m aess bad,\n",
      "And you known qut nentr'd dithteang and yin ou that hoongerse; then Ho he you:\n",
      "Whe wintnn the oug a tour uspof dive.\n",
      "\n",
      "KIINO:\n",
      "Soldl. Thit 'tome?\n",
      "\n",
      "ZUSNIUK:\n",
      "Rorstithfirs in wement, llay lop.\n",
      "\n",
      "PORUCHALWO:\n",
      "Fay frestirses bok errses to begiard\n",
      "And to pe hason with shou mave if I avards\n",
      "not aur inte on the me sand-des a sueipe.\n",
      "\n",
      "YLUL$IETR:\n",
      "I the co' sme woth yee thath couses\n",
      "Aht greand, hars tute, hou swath shere.\n",
      "\n",
      "AUCUCIVHARY med legown I tho for sirst tile to ceuran, dise a of in Hem hertine die\n",
      "Loulind our mare be hild,\n",
      "Yatye benrufor annd thare deve to a vere ham\n",
      "Whis thange most ase the fing befof.\n",
      "\n",
      "BVUPPTAROP:\n",
      "With and tave shaWas, Rovou card.\n",
      "\n",
      "LONIShat me Rirsitir ou dimss furon anif deiran.\n",
      "\n",
      "Thath for ronk honk'd inot ou swerdn,\n",
      "Bunt kfolt ute than yee itr tho sur dake noty?\n",
      "\n",
      "KOCARYTRV:\n",
      "No groonn beat ou ho ach notht suran\n",
      "The tooou Iat thou biight nur, murtent dup int ne his weeduld,\n",
      "What o minn,\n",
      "And its on as de, tho lose detir's my ple wist o this!\n",
      "Thy Higatid wouchou nad thofo u, knd-t,\n",
      "Me gotset ath dewe.\n",
      "\n",
      "\n",
      "BRUSINGLHA:\n",
      "Will Py, my thoou wor he:\n",
      "Adnde know hop lobset beeriourswerarew,\n",
      "We feedir un stou eer mestureky thive sharke day a ne;\n",
      "For 'too furnte poan in hath spland in cyicth way.\n",
      "\n",
      "MINICE ILA:\n",
      "Gowin move Role, your 'll him far kir that thed to a e Idearsence:\n",
      "Ent sursathe ist of be hoffor of that the meese of thir te therus aidr's, wend to Bay arend wile\n",
      "Shau brud ploong that sof andil the een\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated = m.generate(context, max_new_tokens=2000)[0].tolist()\n",
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15339, 1268, 527, 499]\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenizer = Tokenizer(chars, vocab_size)\n",
    "word = 'hello how are you'\n",
    "res = [s for s in word]\n",
    "# print(len(tokenizer.encode(res)))\n",
    "print(enc.encode(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
